{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github Report\n",
    "### https://github.com/ctjong27/Machine_Learning_Sandbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #1\n",
    "### /world_happiness_competition/RandomForestClassifier_Keras_GradiantBoost_practices.ipynb\n",
    "### https://github.com/ctjong27/Machine_Learning_Sandbox/blob/main/world_happiness_competition/RandomForestClassifier_Keras_GradiantBoost_practices.ipynb\n",
    "\n",
    "This notebook is focused on predicting the happiness categories of different countries using the World Happiness AI Model Share competition dataset, which includes features such as GDP and life expectancy. The dataset consists of time-series data, meaning that each data point represents a different country at a specific point in time. The use of time-series data allows the notebook to examine the changes in happiness categories over time, which can provide valuable insights into the factors that contribute to happiness.\n",
    "\n",
    "To analyze the dataset, the notebook explores bivariate results and uses different models to examine the relationships between features and happiness categories. The models used in this notebook include decision tree, random forest, and logistic regression. Each model is designed to identify the features that are most predictive of happiness categories, allowing the notebook to determine which factors have the greatest impact on happiness.\n",
    "\n",
    "The notebook concludes by discussing the models used and their performance, along with relevant hyper-parameter values. Additionally, the notebook stores each of the results created during the project in a subfolder of the Github repository, providing a comprehensive record of the modeling process. Overall, the incorporation of time-series data and the use of various predictive models allows the notebook to effectively analyze the World Happiness AI Model Share competition dataset and make accurate predictions about happiness categories."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #2\n",
    "### /brain_tumor_analysis/#2_COVID_Hackathon_Model_Submission_Notebook.ipynb\n",
    "### https://github.com/ctjong27/Machine_Learning_Sandbox/blob/main/brain_tumor_analysis/%232_COVID_Hackathon_Model_Submission_Notebook.ipynb\n",
    "\n",
    "This Python notebook is a documentation of a image predictive model built using the Covid-19 X-ray of the lungs in identifying COvid positive vs Covid negative patients. The notebook includes visualizations of the dataset and discusses its practical usefulness.\n",
    "\n",
    "The notebook runs at least three prediction models, including one that uses transfer learning. The performance of these models is discussed, along with relevant hyper-parameter values for successful models. The notebook submits the best three models to the leader board for the Covid X-ray Diagnostic AI Model Share competition\n",
    "\n",
    "To conduct image analysis, this Python notebook uses deep learning techniques to extract meaningful features from the Covid-19 X-ray images. The notebook employs transfer learning, which involves using pre-trained models as the starting point for the training process. Transfer learning is an effective technique for image analysis as it can leverage the knowledge learned from large datasets to improve the performance of smaller datasets.\n",
    "\n",
    "The notebook also uses data augmentation techniques to increase the amount of training data available for the model. Data augmentation involves randomly transforming the images during training, such as rotating or flipping the images, to create new data points. This can help the model generalize better to new data and improve its performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #3\n",
    "### /deep_machine_learning/3_deep_learning_submission_notebook.ipynb\n",
    "### https://github.com/ctjong27/Machine_Learning_Sandbox/blob/main/deep_machine_learning/3_deep_learning_submission_notebook.ipynb\n",
    "\n",
    "The third prediction model in the Stanford Sentiment Treebank dataset incorporates time series analysis techniques to make more accurate predictions about the sentiment of a sequence of text over time. This model is designed to work with time series data, which consists of data points collected at regular intervals over time. In this case, the time series data represents a sequence of text that evolves over time, with each data point representing the sentiment of the text at a particular moment.\n",
    "\n",
    "The use of time series analysis techniques is critical for accurately predicting the sentiment of a sequence of text over time. This is because the sentiment of the text at any given moment is likely to be influenced by the sentiment of the text in the past, and understanding these temporal dependencies is key to making accurate predictions. By incorporating LSTM layers into the model, the notebook is able to capture the temporal dependencies of the sentiment data and make more accurate predictions.\n",
    "\n",
    "In addition to the time series analysis techniques, the model also uses an Embedding layer to convert the text input into a numerical representation that the neural network can work with. The use of transfer learning with glove embeddings further improves the model's performance by leveraging pre-trained word embeddings that capture semantic relationships between words. Overall, the incorporation of time series analysis techniques and other deep learning techniques is critical for accurately predicting the sentiment of a sequence of text over time and improving the predictivity of the model on the Stanford Sentiment Treebank dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
